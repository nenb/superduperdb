"use strict";(self.webpackChunknewdocs=self.webpackChunknewdocs||[]).push([[4811],{3905:(e,t,n)=>{n.d(t,{Zo:()=>c,kt:()=>h});var o=n(67294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function a(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);t&&(o=o.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,o)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?a(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):a(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,o,r=function(e,t){if(null==e)return{};var n,o,r={},a=Object.keys(e);for(o=0;o<a.length;o++)n=a[o],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(o=0;o<a.length;o++)n=a[o],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var p=o.createContext({}),l=function(e){var t=o.useContext(p),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},c=function(e){var t=l(e.components);return o.createElement(p.Provider,{value:t},e.children)},u="mdxType",d={inlineCode:"code",wrapper:function(e){var t=e.children;return o.createElement(o.Fragment,{},t)}},m=o.forwardRef((function(e,t){var n=e.components,r=e.mdxType,a=e.originalType,p=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),u=l(n),m=r,h=u["".concat(p,".").concat(m)]||u[m]||d[m]||a;return n?o.createElement(h,i(i({ref:t},c),{},{components:n})):o.createElement(h,i({ref:t},c))}));function h(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var a=n.length,i=new Array(a);i[0]=m;var s={};for(var p in t)hasOwnProperty.call(t,p)&&(s[p]=t[p]);s.originalType=e,s[u]="string"==typeof e?e:r,i[1]=s;for(var l=2;l<a;l++)i[l]=n[l];return o.createElement.apply(null,i)}return o.createElement.apply(null,n)}m.displayName="MDXCreateElement"},77362:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>p,contentTitle:()=>i,default:()=>d,frontMatter:()=>a,metadata:()=>s,toc:()=>l});var o=n(87462),r=(n(67294),n(3905));const a={},i="Run computations on Dask",s={unversionedId:"docs/how_to/parallelize_model_computations_with_dask",id:"docs/how_to/parallelize_model_computations_with_dask",title:"Run computations on Dask",description:"In this example, we show how to run computations on a Dask cluster, rather than in the same process as",source:"@site/content/docs/how_to/parallelize_model_computations_with_dask.md",sourceDirName:"docs/how_to",slug:"/docs/how_to/parallelize_model_computations_with_dask",permalink:"/docs/docs/how_to/parallelize_model_computations_with_dask",draft:!1,editUrl:"https://github.com/SuperDuperDB/superduperdb/tree/main/docs/content/docs/how_to/parallelize_model_computations_with_dask.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Add images, audio or video from URIs",permalink:"/docs/docs/how_to/insert_data_into_superduperdb"},next:{title:"How to connect to and query SuperDuperDB",permalink:"/docs/docs/how_to/playground"}},p={},l=[],c={toc:l},u="wrapper";function d(e){let{components:t,...n}=e;return(0,r.kt)(u,(0,o.Z)({},c,n,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"run-computations-on-dask"},"Run computations on Dask"),(0,r.kt)("p",null,"In this example, we show how to run computations on a Dask cluster, rather than in the same process as\ndata is submitted from. This allows compute to be scaled horizontally, and also submitted to\nworkers, which may utilize specialized hardware, including GPUs."),(0,r.kt)("p",null,"To do this, we need to override the default configuration. To do this, we only need specify the\nconfigurations which diverge from the defaults. In particular, to use a Dask cluster, we specify\n",(0,r.kt)("inlineCode",{parentName:"p"},"CFG.distributed = True")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"!echo '{\"distributed\": true}' > configs.json\n!cat configs.json\n")),(0,r.kt)("p",null,"We can now confirm, by importing the loaded configuration ",(0,r.kt)("inlineCode",{parentName:"p"},"CFG"),", that ",(0,r.kt)("inlineCode",{parentName:"p"},"CFG.distribute == True"),":"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"from superduperdb import CFG\n\nimport pprint\npprint.pprint(CFG.dict())\n")),(0,r.kt)("p",null,"Now that we've set up the environment to use a Dask cluster, we can add some data to the ",(0,r.kt)("inlineCode",{parentName:"p"},"Datalayer"),"."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"from superduperdb.db.base.build import build_datalayer\n\ndb = build_datalayer()\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"db.db.client.drop_database('test_db')\ndb.db.client.drop_database('_filesystem:test_db')\n")),(0,r.kt)("p",null,"As in the previous tutorials, we can wrap models from a range of AI frameworks to interoperate with the data set,\nas well as inserting data with, for instances, tensors of a specific data type:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"import pymongo\nimport torch\n\nfrom superduperdb import superduper\nfrom superduperdb.container.document import Document as D\nfrom superduperdb.ext.torch.tensor import tensor\nfrom superduperdb.db.mongodb.query import Collection\n\nm = superduper(\n    torch.nn.Linear(128, 7),\n    encoder=tensor(torch.float, shape=(7,))\n)\n\nt32 = tensor(torch.float, shape=(128,))\n\noutput = db.execute(\n    Collection('localcluster').insert_many(\n        [D({'x': t32(torch.randn(128))}) for _ in range(1000)], \n        encoders=(t32,)\n    )\n)\n")),(0,r.kt)("p",null,"Now when we instruct the model to make predictions based on the ",(0,r.kt)("inlineCode",{parentName:"p"},"Datalayer"),", the computations run on the Dask cluster. The ",(0,r.kt)("inlineCode",{parentName:"p"},".predict")," method returns a ",(0,r.kt)("inlineCode",{parentName:"p"},"Job")," instance, which can be used to monitor the progress of the computation:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"job = m.predict(\n    X='x',\n    db=db,\n    select=Collection('localcluster').find(),\n)\n\njob.watch()\n")),(0,r.kt)("p",null,"To check that the ",(0,r.kt)("inlineCode",{parentName:"p"},"Datalayer")," has been populated with outputs, we can check the ",(0,r.kt)("inlineCode",{parentName:"p"},'"_outputs"')," field of a record:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"db.execute(Collection('localcluster').find_one()).unpack()\n")))}d.isMDXComponent=!0}}]);