"use strict";(self.webpackChunknewdocs=self.webpackChunknewdocs||[]).push([[524],{3905:(e,n,t)=>{t.d(n,{Zo:()=>p,kt:()=>f});var r=t(67294);function a(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function o(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);n&&(r=r.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,r)}return t}function i(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?o(Object(t),!0).forEach((function(n){a(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):o(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function s(e,n){if(null==e)return{};var t,r,a=function(e,n){if(null==e)return{};var t,r,a={},o=Object.keys(e);for(r=0;r<o.length;r++)t=o[r],n.indexOf(t)>=0||(a[t]=e[t]);return a}(e,n);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(r=0;r<o.length;r++)t=o[r],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(a[t]=e[t])}return a}var l=r.createContext({}),c=function(e){var n=r.useContext(l),t=n;return e&&(t="function"==typeof e?e(n):i(i({},n),e)),t},p=function(e){var n=c(e.components);return r.createElement(l.Provider,{value:n},e.children)},d="mdxType",u={inlineCode:"code",wrapper:function(e){var n=e.children;return r.createElement(r.Fragment,{},n)}},m=r.forwardRef((function(e,n){var t=e.components,a=e.mdxType,o=e.originalType,l=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),d=c(t),m=a,f=d["".concat(l,".").concat(m)]||d[m]||u[m]||o;return t?r.createElement(f,i(i({ref:n},p),{},{components:t})):r.createElement(f,i({ref:n},p))}));function f(e,n){var t=arguments,a=n&&n.mdxType;if("string"==typeof e||a){var o=t.length,i=new Array(o);i[0]=m;var s={};for(var l in n)hasOwnProperty.call(n,l)&&(s[l]=n[l]);s.originalType=e,s[d]="string"==typeof e?e:a,i[1]=s;for(var c=2;c<o;c++)i[c]=t[c];return r.createElement.apply(null,i)}return r.createElement.apply(null,t)}m.displayName="MDXCreateElement"},95358:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>i,default:()=>u,frontMatter:()=>o,metadata:()=>s,toc:()=>c});var r=t(87462),a=(t(67294),t(3905));const o={},i="Transfer learning using Sentence Transformers and Scikit-Learn",s={unversionedId:"use_cases/items/transfer_learning",id:"use_cases/items/transfer_learning",title:"Transfer learning using Sentence Transformers and Scikit-Learn",description:"In this example, we'll be demonstrating how to simply implement transfer learning using SuperDuperDB.",source:"@site/content/use_cases/items/transfer_learning.md",sourceDirName:"use_cases/items",slug:"/use_cases/items/transfer_learning",permalink:"/docs/use_cases/items/transfer_learning",draft:!1,editUrl:"https://github.com/SuperDuperDB/superduperdb/tree/main/docs/content/use_cases/items/transfer_learning.md",tags:[],version:"current",frontMatter:{},sidebar:"useCasesSidebar",previous:{title:"Sentiment analysis with transformers",permalink:"/docs/use_cases/items/sentiment_analysis_use_case"},next:{title:"Cataloguing voice-memos for a self managed personal assistant",permalink:"/docs/use_cases/items/voice_memos"}},l={},c=[],p={toc:c},d="wrapper";function u(e){let{components:n,...t}=e;return(0,a.kt)(d,(0,r.Z)({},p,t,{components:n,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"transfer-learning-using-sentence-transformers-and-scikit-learn"},"Transfer learning using Sentence Transformers and Scikit-Learn"),(0,a.kt)("p",null,"In this example, we'll be demonstrating how to simply implement transfer learning using SuperDuperDB.\nYou'll find related examples on vector-search and simple training examples using scikit-learn in the\nthe notebooks directory of the project. Transfer learning leverages similar components, and may be used synergistically with vector-search. Vectors are, after all, simultaneously featurizations of\ndata and may be used in downstream learning tasks."),(0,a.kt)("p",null,"Let's first connect to MongoDB via SuperDuperDB, you read explanations of how to do this in\nthe docs, and in the ",(0,a.kt)("inlineCode",{parentName:"p"},"notebooks/")," directory."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"from superduperdb import superduper\nfrom superduperdb.db.mongodb.query import Collection\nimport pymongo\n\ndb = superduper(\n    pymongo.MongoClient().documents\n)\n\ncollection = Collection('transfer')\n")),(0,a.kt)("p",null,"We'll use textual data labelled with sentiment, to test the functionality. Transfer learning\ncan be used on any data which can be processed with SuperDuperDB models."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"import numpy\nfrom datasets import load_dataset\n\nfrom superduperdb.container.document import Document as D\n\ndata = load_dataset(\"imdb\")\n\ntrain_data = [\n    D({'_fold': 'train', **data['train'][int(i)]}) \n    for i in numpy.random.permutation(len(data['train']))\n][:5000]\n\nvalid_data = [\n    D({'_fold': 'valid', **data['test'][int(i)]}) \n    for i in numpy.random.permutation(len(data['test']))\n][:500]\n\ndb.execute(collection.insert_many(train_data))\n\nr = db.execute(collection.find_one())\nr\n")),(0,a.kt)("p",null,"Let's create a SuperDuperDB model based on a ",(0,a.kt)("inlineCode",{parentName:"p"},"sentence_transformers")," model.\nYou'll notice that we don't necessarily need a native SuperDuperDB integration to a model library\nin order to leverage its power with SuperDuperDB. For example, in this case, we just need\nto configure the ",(0,a.kt)("inlineCode",{parentName:"p"},"Model")," wrapper to interoperate correctly with the ",(0,a.kt)("inlineCode",{parentName:"p"},"SentenceTransformer")," class. After doing this, we can link the model to a collection, and /docs/docs/usage/models#daemonizing-models-with-listeners the model using the ",(0,a.kt)("inlineCode",{parentName:"p"},"listen=True")," keyword:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"from superduperdb.container.model import Model\nimport sentence_transformers\n\nfrom superduperdb.ext.numpy.array import array\n\nm = Model(\n    identifier='all-MiniLM-L6-v2',\n    object=sentence_transformers.SentenceTransformer('all-MiniLM-L6-v2'),\n    encoder=array('float32', shape=(384,)),\n    predict_method='encode',\n    batch_predict=True,\n)\n\nm.predict(\n    X='text',\n    db=db,\n    select=collection.find(),\n    listen=True\n)\n")),(0,a.kt)("p",null,"Now that we've created and added the model which computes features for the ",(0,a.kt)("inlineCode",{parentName:"p"},'"text"'),", we can train a\ndownstream model using Scikit-Learn:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"from sklearn.svm import SVC\n\nmodel = superduper(\n    SVC(gamma='scale', class_weight='balanced', C=100, verbose=True),\n    postprocess=lambda x: int(x)\n)\n\nmodel.fit(\n    X='text',\n    y='label',\n    db=db,\n    select=collection.find().featurize({'text': 'all-MiniLM-L6-v2'}),\n)\n")),(0,a.kt)("p",null,"Now that the model has been trained, we can apply the model to the database, also daemonizing the model\nwith ",(0,a.kt)("inlineCode",{parentName:"p"},"listen=True"),"."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"model.predict(\n    X='text',\n    db=db,\n    select=collection.find().featurize({'text': 'all-MiniLM-L6-v2'}),\n    listen=True,\n)\n")),(0,a.kt)("p",null,"To verify that this process has worked, we can sample a few records, to inspect the sanity of the predictions"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"r = next(db.execute(collection.aggregate([{'$sample': {'size': 1}}])))\nprint(r['text'][:100])\nprint(r['_outputs']['text']['svc'])\n")))}u.isMDXComponent=!0}}]);