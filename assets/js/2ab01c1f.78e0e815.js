"use strict";(self.webpackChunknewdocs=self.webpackChunknewdocs||[]).push([[445],{3905:(e,n,t)=>{t.d(n,{Zo:()=>c,kt:()=>y});var r=t(67294);function a(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function i(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);n&&(r=r.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,r)}return t}function o(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?i(Object(t),!0).forEach((function(n){a(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):i(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function s(e,n){if(null==e)return{};var t,r,a=function(e,n){if(null==e)return{};var t,r,a={},i=Object.keys(e);for(r=0;r<i.length;r++)t=i[r],n.indexOf(t)>=0||(a[t]=e[t]);return a}(e,n);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(r=0;r<i.length;r++)t=i[r],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(a[t]=e[t])}return a}var l=r.createContext({}),p=function(e){var n=r.useContext(l),t=n;return e&&(t="function"==typeof e?e(n):o(o({},n),e)),t},c=function(e){var n=p(e.components);return r.createElement(l.Provider,{value:n},e.children)},m="mdxType",u={inlineCode:"code",wrapper:function(e){var n=e.children;return r.createElement(r.Fragment,{},n)}},d=r.forwardRef((function(e,n){var t=e.components,a=e.mdxType,i=e.originalType,l=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),m=p(t),d=a,y=m["".concat(l,".").concat(d)]||m[d]||u[d]||i;return t?r.createElement(y,o(o({ref:n},c),{},{components:t})):r.createElement(y,o({ref:n},c))}));function y(e,n){var t=arguments,a=n&&n.mdxType;if("string"==typeof e||a){var i=t.length,o=new Array(i);o[0]=d;var s={};for(var l in n)hasOwnProperty.call(n,l)&&(s[l]=n[l]);s.originalType=e,s[m]="string"==typeof e?e:a,o[1]=s;for(var p=2;p<i;p++)o[p]=t[p];return r.createElement.apply(null,o)}return r.createElement.apply(null,t)}d.displayName="MDXCreateElement"},27533:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>u,frontMatter:()=>i,metadata:()=>s,toc:()=>p});var r=t(87462),a=(t(67294),t(3905));const i={},o="MNIST using scikit-learn and SuperDuperDB",s={unversionedId:"use_cases/items/mnist_sklearn",id:"use_cases/items/mnist_sklearn",title:"MNIST using scikit-learn and SuperDuperDB",description:"In a previous example we discussed how to implement MNIST classification with CNNs in torch",source:"@site/content/use_cases/items/mnist_sklearn.md",sourceDirName:"use_cases/items",slug:"/use_cases/items/mnist_sklearn",permalink:"/docs/use_cases/items/mnist_sklearn",draft:!1,editUrl:"https://github.com/SuperDuperDB/superduperdb/tree/main/docs/content/use_cases/items/mnist_sklearn.md",tags:[],version:"current",frontMatter:{},sidebar:"useCasesSidebar",previous:{title:"Turn your classical-database into a vector-database with SuperDuperDB",permalink:"/docs/use_cases/items/compare_vector_search_solutions"},next:{title:"Training and maintaining MNIST predictions",permalink:"/docs/use_cases/items/mnist_torch"}},l={},p=[],c={toc:p},m="wrapper";function u(e){let{components:n,...t}=e;return(0,a.kt)(m,(0,r.Z)({},c,t,{components:n,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"mnist-using-scikit-learn-and-superduperdb"},"MNIST using scikit-learn and SuperDuperDB"),(0,a.kt)("p",null,"In a ",(0,a.kt)("a",{parentName:"p",href:"mnist_torch.html"},"previous example")," we discussed how to implement MNIST classification with CNNs in ",(0,a.kt)("inlineCode",{parentName:"p"},"torch"),"\nusing SuperDuperDB. "),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"from sklearn.datasets import fetch_openml\nfrom sklearn.metrics import accuracy_score,classification_report\nimport numpy as np\nfrom sklearn import svm\n")),(0,a.kt)("p",null,"As before we'll import the python MongoDB client ",(0,a.kt)("inlineCode",{parentName:"p"},"pymongo"),'\nand "wrap" our database to convert it to a SuperDuper ',(0,a.kt)("inlineCode",{parentName:"p"},"Datalayer"),":"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"import pymongo\nfrom superduperdb import superduper\n\ndb = pymongo.MongoClient().documents\n\ndb = superduper(db)\n")),(0,a.kt)("p",null,"Similarly to last time, we can add data to SuperDuperDB in a way which very similar to using ",(0,a.kt)("inlineCode",{parentName:"p"},"pymongo"),".\nThis time, we'll add the data as ",(0,a.kt)("inlineCode",{parentName:"p"},"numpy.array")," to SuperDuperDB, using the ",(0,a.kt)("inlineCode",{parentName:"p"},"Document-Encoder")," formalism:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"from superduperdb.ext.numpy.array import array\nfrom superduperdb.container.document import Document as D\nfrom superduperdb.db.mongodb.query import Collection\n\nmnist = fetch_openml('mnist_784')\nix = np.random.permutation(10000)\nX = np.array(mnist.data)[ix, :]\ny = np.array(mnist.target)[ix].astype(int)\n\na = array('float64', shape=(784,))\n\ncollection = Collection(name='mnist')\n\ndata = [D({'img': a(X[i]), 'class': int(y[i])}) for i in range(len(X))]\n\ndb.execute(\n    collection.insert_many(data, encoders=[a])\n)\n")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"db.execute(collection.find_one())\n")),(0,a.kt)("p",null,"Models are built similarly to the ",(0,a.kt)("inlineCode",{parentName:"p"},"Datalayer"),", by wrapping a standard Python-AI-ecosystem model:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"model = superduper(\n    svm.SVC(gamma='scale', class_weight='balanced', C=100, verbose=True),\n    postprocess=lambda x: int(x)\n)\n")),(0,a.kt)("p",null,"Now let's fit the model. The optimization uses Scikit-Learn's inbuilt training procedures.\nUnlike in a standard ",(0,a.kt)("inlineCode",{parentName:"p"},"sklearn")," use-case, we don't need to fetch the data client side. Instead,\nwe simply name the fields in the MongoDB collection which we'd like to use."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"model.fit(X='img', y='class', db=db, select=collection.find())\n")),(0,a.kt)("p",null,"Installed models and functionality can be viewed using ",(0,a.kt)("inlineCode",{parentName:"p"},"db.show"),":"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"db.show('model')\n")),(0,a.kt)("p",null,"The model may be reloaded in another session from the database.\nAs with ",(0,a.kt)("inlineCode",{parentName:"p"},".fit"),", the model may be applied to data in the database with ",(0,a.kt)("inlineCode",{parentName:"p"},".predict"),":"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"m = db.load('model', 'svc')\nm.predict(X='img', db=db, select=collection.find(), max_chunk_size=3000)\n")),(0,a.kt)("p",null,"We can verify that the predictions make sense by fetching a few random data-points:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"r = next(db.execute(collection.aggregate([{'$match': {'_fold': 'valid'}} ,{'$sample': {'size': 1}}])))\nprint(r['class'])\nprint(r['_outputs'])\n")))}u.isMDXComponent=!0}}]);